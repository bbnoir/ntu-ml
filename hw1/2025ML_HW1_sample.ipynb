{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz0_QVkxCrX3"
      },
      "source": [
        "# **2025 ML FALL HW1: PM2.5 Prediction (Regression)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeZnPAiwDRWG"
      },
      "source": [
        "Author: MLTAs\n",
        "\n",
        "Methods:\n",
        "* Training with all data\n",
        "* Optimizer: RMSProp (default)\n",
        "* TODOs:\n",
        "  - Complete the `valid()` function\n",
        "  - Tune the hyperparameters in `train_config`\n",
        "  - Implement 2nd-order polynomial regression model (without interaction terms) in `minibatch_2()`\n",
        "  - Implement feature normalization in `normalize_train_data()`\n",
        "  - Feature selection\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS_4-77xHk44"
      },
      "source": [
        "# **Import Some Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-onQd4JNA5H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import math\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqMEWsRekx0L"
      },
      "source": [
        "# **Fix random seed**\n",
        "\n",
        "\n",
        "This is for the reproduction of your result. **DO NOT modify this secton!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxDA6fJb_Uem"
      },
      "outputs": [],
      "source": [
        "seed = 9487\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OVRMuTAc1_E"
      },
      "source": [
        "# **Download training data**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0Zo8JUp5kJ4",
        "outputId": "b9204435-62e0-4946-a0cf-a8d094051c25"
      },
      "outputs": [],
      "source": [
        "# !gdown --id \"1Hfzrcm69QwdFvdeF0uASoQlcVxKw_hHy\" --output \"train.csv\"\n",
        "# !gdown --id '155N6fzI7vAFzHAGdy6jkaWIksWH6Y1G2' --output \"test.csv\"\n",
        "\n",
        "# Incase the links above die, you can use the following instead.\n",
        "#!gdown --id '11abE854Eyv4BA7qt5k8r_80sJ3KuOQUN' --output \"train.csv\"\n",
        "#!gdown --id '1uod-Z4ztluXnuHtgUbm39nMudUKqXHMl' --output \"test.csv\"\n",
        "\n",
        "# If the data is still missing, you can manually download it from kaggle, and upload the files under /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHpuZmQwXpz8"
      },
      "outputs": [],
      "source": [
        "def valid(x, y, y_mean, y_std):\n",
        "  # TODO: Try to filter out extreme values.\n",
        "  # Exclude data points with y over 2.5 standard deviations from the mean\n",
        "\n",
        "  std_thresh = 4\n",
        "  if y > y_mean + std_thresh * y_std or y < y_mean - std_thresh * y_std:\n",
        "    return False\n",
        "\n",
        "  return True\n",
        "\n",
        "\n",
        "# Create your dataset\n",
        "def parse2train(data, feats):\n",
        "\n",
        "  x = []\n",
        "  y = []\n",
        "\n",
        "  # Calculate mean and std of y for filtering\n",
        "  y_all = data[-1, 8:]  # All PM2.5 values\n",
        "  y_mean = np.mean(y_all)\n",
        "  y_std = np.std(y_all)\n",
        "\n",
        "  # Use data #0~#7 to predict #8 => Total data length should be decresased by 8.\n",
        "  total_length = data.shape[1] - 8\n",
        "\n",
        "  for i in range(total_length):\n",
        "    x_tmp = data[feats, i:i+8] # Use data #0~#7 to predict #8, data #1~#8 to predict #9, etc.\n",
        "    y_tmp = data[-1, i+8] # last column of (i+8)th row: PM2.5\n",
        "\n",
        "    # Filter out extreme values to train.\n",
        "    if valid(x_tmp, y_tmp, y_mean, y_std):\n",
        "      x.append(x_tmp.reshape(-1,))\n",
        "      y.append(y_tmp)\n",
        "\n",
        "  # x.shape: (n, 15, 8)\n",
        "  # y.shape: (n, 1)\n",
        "  x = np.array(x)\n",
        "  y = np.array(y)\n",
        "\n",
        "  return x,y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyEpvVVQdZ0c"
      },
      "source": [
        "#**Gradient descent**\n",
        "###**RMSProp**\n",
        "1. $v_t=\\beta \\cdot v_{t-1} + (1-\\beta)(\\nabla w_t)^2$\n",
        "2. $w_{t+1}=w_t - \\frac{\\eta}{\\sqrt{(v_t)}+\\epsilon}\\nabla w_t$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* This is our gradient descent algorithm. RMSProp was implemented in `minibatch()`.\n",
        "* You can implement other algorithm, such as SGD or other gradient descent variants listed below, which may (or may not) improve performance.\n",
        "* However, **modules like sklearn and pytorch are not allowed!!!**\n",
        "* Ref:\n",
        "  - Prof. G. Hinton's lecture: https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\n",
        "  - Prof. Hung-Yi Lee's video: https://youtu.be/HYUXEeh3kwY?si=RtLjSj51WK1pmz87\n",
        "\n",
        "###**Adam (RMSProp + Momemtum)**\n",
        "* Ref:\n",
        "  - Paper: https://arxiv.org/pdf/1412.6980\n",
        "  - Prof. Hung-Yi Lee's video: https://youtu.be/HYUXEeh3kwY?si=RtLjSj51WK1pmz87\n",
        "\n",
        "###**AdamW (Adam with decoupled weight decay)**\n",
        "* Ref:\n",
        "  - Paper: https://arxiv.org/pdf/1711.05101\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcofxZ8c4kZE"
      },
      "outputs": [],
      "source": [
        "def minibatch(x, y, config):\n",
        "    # Randomize the data in minibatch\n",
        "    index = np.arange(x.shape[0])\n",
        "    np.random.shuffle(index)\n",
        "    x = x[index]\n",
        "    y = y[index]\n",
        "\n",
        "    # Initialization\n",
        "    batch_size = config.batch_size\n",
        "    lr = config.lr\n",
        "    epoch = config.epoch\n",
        "    decay_rate = config.decay_rate\n",
        "    epsilon = 1e-8\n",
        "\n",
        "    # Linear regression: only contains two parameters (w, b).\n",
        "    w = np.full(x[0].shape, 0.1).reshape(-1, 1)\n",
        "    w\n",
        "    bias = 0.1\n",
        "\n",
        "    # Optimizer states\n",
        "    cache_w = np.zeros_like(w)\n",
        "    cache_b = 0.0\n",
        "\n",
        "    # Training loop\n",
        "    for num in range(epoch):\n",
        "        epoch_loss = 0\n",
        "        count = 0\n",
        "        for b in range(int(x.shape[0] / batch_size)):\n",
        "            x_batch = x[b * batch_size:(b + 1) * batch_size]\n",
        "            y_batch = y[b * batch_size:(b + 1) * batch_size].reshape(-1, 1)\n",
        "\n",
        "            # Prediction of linear regression\n",
        "            pred = np.dot(x_batch, w) + bias\n",
        "\n",
        "            # Loss\n",
        "            loss = y_batch - pred\n",
        "\n",
        "            # Compute gradient\n",
        "            g_t = np.dot(x_batch.transpose(), loss) * (-2)\n",
        "            g_t_b = loss.sum(axis=0) * (-2)\n",
        "\n",
        "            # Update cache\n",
        "            cache_w = decay_rate * cache_w + (1 - decay_rate) * g_t**2\n",
        "            cache_b = decay_rate * cache_b + (1 - decay_rate) * g_t_b**2\n",
        "\n",
        "            # Update weight & bias\n",
        "            w -= lr * g_t / (np.sqrt(cache_w) + epsilon)\n",
        "            bias -= lr * g_t_b / (np.sqrt(cache_b) + epsilon)\n",
        "\n",
        "            # Accumulate loss for RMSE\n",
        "            epoch_loss += np.sum(loss ** 2)\n",
        "            count += y_batch.shape[0]\n",
        "\n",
        "        rmse = np.sqrt(epoch_loss / count)\n",
        "        print(f\"Epoch {num+1}/{epoch}, RMSE: {rmse:.4f}\")\n",
        "\n",
        "    return w, bias\n",
        "\n",
        "# TODO: Implement 2-nd polynomial regression version for the report.\n",
        "def minibatch_2(x, y, config):\n",
        "    # Randomize the data in minibatch\n",
        "    index = np.arange(x.shape[0])\n",
        "    np.random.shuffle(index)\n",
        "    x = x[index]\n",
        "    y = y[index]\n",
        "    \n",
        "    # Initialization\n",
        "    batch_size = config.batch_size\n",
        "    lr = config.lr\n",
        "    epoch = config.epoch\n",
        "    decay_rate = config.decay_rate\n",
        "    epsilon = 1e-8\n",
        "    \n",
        "    # Polynomial regression\n",
        "    w2 = np.full(x[0].shape, 0.1).reshape(-1, 1)\n",
        "    w1 = np.full(x[0].shape, 0.1).reshape(-1, 1)\n",
        "    bias = 0.1\n",
        "\n",
        "    # Optimizer states\n",
        "    cache_w2 = np.zeros_like(w2)\n",
        "    cache_w1 = np.zeros_like(w1)\n",
        "    cache_b = 0.0\n",
        "    \n",
        "    # Training loop\n",
        "    for num in range(epoch):\n",
        "        epoch_loss = 0\n",
        "        count = 0\n",
        "        for b in range(int(x.shape[0] / batch_size)):\n",
        "            x_batch = x[b * batch_size:(b + 1) * batch_size]\n",
        "            y_batch = y[b * batch_size:(b + 1) * batch_size].reshape(-1, 1)\n",
        "            \n",
        "            # Prediction of polynomial regression\n",
        "            pred = np.dot(x_batch**2, w2) + np.dot(x_batch, w1) + bias\n",
        "            \n",
        "            # Loss\n",
        "            loss = y_batch - pred\n",
        "            \n",
        "            # Compute gradient - Fixed the gradient computation\n",
        "            g_t2 = np.dot((x_batch**2).transpose(), loss) * (-2)  # Gradient for w2\n",
        "            g_t1 = np.dot(x_batch.transpose(), loss) * (-2)      # Gradient for w1\n",
        "            g_t_b = loss.sum(axis=0) * (-2)                      # Gradient for bias\n",
        "            \n",
        "            # Update cache\n",
        "            cache_w2 = decay_rate * cache_w2 + (1 - decay_rate) * g_t2**2\n",
        "            cache_w1 = decay_rate * cache_w1 + (1 - decay_rate) * g_t1**2\n",
        "            cache_b = decay_rate * cache_b + (1 - decay_rate) * g_t_b**2\n",
        "            \n",
        "            # Update weights & bias\n",
        "            w2 -= lr * g_t2 / (np.sqrt(cache_w2) + epsilon)\n",
        "            w1 -= lr * g_t1 / (np.sqrt(cache_w1) + epsilon)\n",
        "            bias -= lr * g_t_b / (np.sqrt(cache_b) + epsilon)\n",
        "            \n",
        "            # Accumulate loss for RMSE\n",
        "            epoch_loss += np.sum(loss ** 2)\n",
        "            count += y_batch.shape[0]\n",
        "        rmse = np.sqrt(epoch_loss / count)\n",
        "        print(f\"Epoch {num+1}/{epoch}, RMSE: {rmse:.4f}\")\n",
        "    \n",
        "    # Calculate RMSE on the entire training set\n",
        "    total_pred = np.dot(x**2, w2) + np.dot(x, w1) + bias\n",
        "    total_loss = y.reshape(-1, 1) - total_pred\n",
        "    total_rmse = np.sqrt(np.mean(total_loss ** 2))\n",
        "    print(f\"Total Training RMSE: {total_rmse:.4f}\")\n",
        "    return w2, w1, bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpdOsMfXLxH2"
      },
      "outputs": [],
      "source": [
        "from argparse import Namespace\n",
        "\n",
        "# TODO: Tune the config to boost your performance.\n",
        "train_config = Namespace(\n",
        "    batch_size = 64,\n",
        "    lr = 0.001,\n",
        "    epoch = 1000,\n",
        "    decay_rate = 0.99\n",
        ")\n",
        "\n",
        "use_norm = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ay-RhqqA88vS"
      },
      "source": [
        "# **Training your regression model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "EoR5Q5kvJm4t",
        "outputId": "5e24e2ff-04ef-4f18-acd7-755d5ba6371f"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"train.csv\")\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RbM6K-e6dTz"
      },
      "outputs": [],
      "source": [
        "# TODO: Normalize each column (except PM2.5) for the report (use z-score normalization)\n",
        "def normalize_train_data(df):\n",
        "    \"\"\"\n",
        "    Steps:\n",
        "    1. For each column (except PM2.5): calculate mean and std\n",
        "    2. Apply standardization: (column - mean) / std\n",
        "    3. Store normalization parameters for later use on test data\n",
        "\n",
        "    Returns:\n",
        "        normalized_df: DataFrame with normalized features\n",
        "        norm_params: Dict with {'column': {'mean': X, 'std': Y}}\n",
        "\n",
        "    Hint: Loop through data.columns, skip PM2.5\n",
        "    \"\"\"\n",
        "    # Your implementation here\n",
        "    normalize_df = df.copy()\n",
        "    norm_params = {}\n",
        "    for col in df.columns:\n",
        "        if col != 'PM2.5':\n",
        "            mean = df[col].mean()\n",
        "            std = df[col].std()\n",
        "            normalize_df[col] = (df[col] - mean) / std\n",
        "            norm_params[col] = {'mean': mean, 'std': std}\n",
        "    return normalize_df, norm_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Akqj5yYVGHA"
      },
      "outputs": [],
      "source": [
        "# Choose your features to train.\n",
        "# Hint:\n",
        "# 1. You can select more than one feature.\n",
        "# 2. You should select \"good\" features.\n",
        "\n",
        "# TODO: Carefully justify which feature should be chosen.\n",
        "# feats = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
        "feats = [6, 14]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiEWGMQXLM99"
      },
      "outputs": [],
      "source": [
        "# Training data preprocessing\n",
        "def train_processing(train_df, norm=False):\n",
        "    \"\"\"Process training train_df with optional normalization\"\"\"\n",
        "\n",
        "    if norm:\n",
        "        # Normalize training data and save parameters (mean & std)\n",
        "        data_norm, norm_params = normalize_train_data(train_df)\n",
        "        data_values = data_norm.values\n",
        "    else:\n",
        "        # Use raw training data\n",
        "        data_values = train_df.values\n",
        "        norm_params = None\n",
        "\n",
        "    # Common processing steps\n",
        "    train_data = np.transpose(np.array(np.float64(data_values)))\n",
        "    train_x, train_y = parse2train(train_data, feats)\n",
        "\n",
        "    return train_x, train_y, norm_params\n",
        "\n",
        "train_x, train_y, norm_params = train_processing(train_df, norm=use_norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze the training data by calculating correlation between train_x and train_y\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# First, let's check what we're actually working with\n",
        "print(\"Current feats selection:\", feats)\n",
        "print(\"Length of feats:\", len(feats))\n",
        "print(\"Shape of train_x:\", train_x.shape)\n",
        "print(\"Shape of train_y:\", train_y.shape)\n",
        "print()\n",
        "\n",
        "# Let's look at the actual data structure\n",
        "print(\"First, let's see what columns are in the original data:\")\n",
        "print(\"Train CSV columns:\", list(train_df.columns))\n",
        "print(\"Number of columns:\", len(train_df.columns))\n",
        "print()\n",
        "\n",
        "# Feature names corresponding to indices 0-17 (18 features total in the CSV)\n",
        "feature_names = list(train_df.columns[:-1])  # All except PM2.5\n",
        "print(\"Available feature names:\", feature_names)\n",
        "print()\n",
        "\n",
        "# Calculate correlation between each feature and target\n",
        "timesteps = 8\n",
        "n_selected_features = len(feats)\n",
        "\n",
        "correlations = []\n",
        "feature_correlations = {}\n",
        "\n",
        "print(\"Correlation analysis between features and PM2.5:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Calculate correlation for each selected feature across all timesteps\n",
        "for i, feat_idx in enumerate(feats):\n",
        "    if feat_idx < len(feature_names):\n",
        "        feature_name = feature_names[feat_idx]\n",
        "    else:\n",
        "        feature_name = f'Feature_{feat_idx}'\n",
        "    \n",
        "    # Extract all timesteps for this feature\n",
        "    start_idx = i * timesteps\n",
        "    end_idx = (i + 1) * timesteps\n",
        "    feature_data = train_x[:, start_idx:end_idx]\n",
        "    \n",
        "    # Calculate correlation for each timestep\n",
        "    timestep_corrs = []\n",
        "    for t in range(timesteps):\n",
        "        corr = np.corrcoef(feature_data[:, t], train_y)[0, 1]\n",
        "        if np.isnan(corr):  # Handle constant features\n",
        "            corr = 0.0\n",
        "        timestep_corrs.append(corr)\n",
        "    \n",
        "    # Average correlation across timesteps\n",
        "    avg_corr = np.mean(timestep_corrs)\n",
        "    max_corr = np.max(np.abs(timestep_corrs))\n",
        "    \n",
        "    feature_correlations[feature_name] = {\n",
        "        'avg_correlation': avg_corr,\n",
        "        'max_abs_correlation': max_corr,\n",
        "        'timestep_correlations': timestep_corrs,\n",
        "        'feature_idx': feat_idx\n",
        "    }\n",
        "    \n",
        "    print(f\"{feature_name:15s} (idx {feat_idx:2d}): Avg={avg_corr:7.3f}, Max_abs={max_corr:7.3f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Top 10 features by average absolute correlation:\")\n",
        "sorted_features = sorted(feature_correlations.items(), \n",
        "                        key=lambda x: abs(x[1]['avg_correlation']), \n",
        "                        reverse=True)\n",
        "\n",
        "for i, (name, stats) in enumerate(sorted_features[:10]):\n",
        "    print(f\"{i+1:2d}. {name:15s} (idx {stats['feature_idx']:2d}): {stats['avg_correlation']:7.3f}\")\n",
        "\n",
        "print(\"\\nTop 10 features by maximum absolute correlation:\")\n",
        "sorted_by_max = sorted(feature_correlations.items(), \n",
        "                      key=lambda x: x[1]['max_abs_correlation'], \n",
        "                      reverse=True)\n",
        "\n",
        "for i, (name, stats) in enumerate(sorted_by_max[:10]):\n",
        "    print(f\"{i+1:2d}. {name:15s} (idx {stats['feature_idx']:2d}): {stats['max_abs_correlation']:7.3f}\")\n",
        "\n",
        "# Create a correlation heatmap for better visualization\n",
        "plt.figure(figsize=(15, 10))\n",
        "corr_matrix = np.zeros((len(feats), timesteps))\n",
        "\n",
        "for i, feat_idx in enumerate(feats):\n",
        "    if feat_idx < len(feature_names):\n",
        "        feature_name = feature_names[feat_idx]\n",
        "    else:\n",
        "        feature_name = f'Feature_{feat_idx}'\n",
        "        \n",
        "    start_idx = i * timesteps\n",
        "    end_idx = (i + 1) * timesteps\n",
        "    feature_data = train_x[:, start_idx:end_idx]\n",
        "    \n",
        "    for t in range(timesteps):\n",
        "        corr = np.corrcoef(feature_data[:, t], train_y)[0, 1]\n",
        "        if np.isnan(corr):  # Handle constant features\n",
        "            corr = 0.0\n",
        "        corr_matrix[i, t] = corr\n",
        "\n",
        "# Plot heatmap\n",
        "plt.imshow(corr_matrix, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)\n",
        "plt.colorbar(label='Correlation Coefficient')\n",
        "plt.title('Correlation between Features and PM2.5 across Time Steps')\n",
        "plt.xlabel('Time Step (t-7 to t)')\n",
        "plt.ylabel('Features')\n",
        "\n",
        "# Set y-axis labels\n",
        "y_labels = [feature_names[feat_idx] if feat_idx < len(feature_names) else f'Feature_{feat_idx}' \n",
        "           for feat_idx in feats]\n",
        "plt.yticks(range(len(feats)), y_labels, rotation=0)\n",
        "plt.xticks(range(timesteps), [f't-{7-i}' for i in range(timesteps)])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nAnalysis Summary:\")\n",
        "print(f\"- Total samples: {train_x.shape[0]}\")\n",
        "print(f\"- Selected features: {len(feats)} features × {timesteps} timesteps = {train_x.shape[1]} dimensions\")\n",
        "print(f\"- Target (PM2.5) range: [{train_y.min():.2f}, {train_y.max():.2f}]\")\n",
        "print(f\"- Target (PM2.5) mean: {train_y.mean():.2f}, std: {train_y.std():.2f}\")\n",
        "\n",
        "# Additional statistics\n",
        "print(f\"\\nFeature statistics:\")\n",
        "strong_features = [name for name, stats in feature_correlations.items() \n",
        "                  if abs(stats['avg_correlation']) > 0.3]\n",
        "moderate_features = [name for name, stats in feature_correlations.items() \n",
        "                    if 0.1 < abs(stats['avg_correlation']) <= 0.3]\n",
        "weak_features = [name for name, stats in feature_correlations.items() \n",
        "                if abs(stats['avg_correlation']) <= 0.1]\n",
        "\n",
        "print(f\"- Strong correlation (>0.3): {len(strong_features)} features: {strong_features}\")\n",
        "print(f\"- Moderate correlation (0.1-0.3): {len(moderate_features)} features: {moderate_features}\")\n",
        "print(f\"- Weak correlation (≤0.1): {len(weak_features)} features: {weak_features}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot y vs index to visualize distribution and save in high resolution\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(train_y, marker='.', linestyle='-', markersize=1, alpha=0.5)\n",
        "plt.title('Distribution of PM2.5 Values in Training Data')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('PM2.5')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('pm25_distribution.png', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "w, bias = minibatch(train_x, train_y, train_config)\n",
        "print(w.shape, bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Plot feature to PM2.5 scatter plot\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.scatter(train_df[\"NO\"], train_df[\"PM2.5\"], alpha=0.5)\n",
        "# plt.title('Feature vs PM2.5 Scatter Plot')\n",
        "# plt.xlabel('Feature Value')\n",
        "# plt.ylabel('PM2.5 Value')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Plot each feature to PM2.5 scatter plot and save the figures\n",
        "# for feature in train_df.columns:\n",
        "#     if feature != 'PM2.5':\n",
        "#         plt.figure(figsize=(10, 6))\n",
        "#         plt.scatter(train_df[feature], train_df[\"PM2.5\"], alpha=0.5)\n",
        "#         plt.title(f'{feature} vs PM2.5 Scatter Plot')\n",
        "#         plt.xlabel(f'{feature} Value')\n",
        "#         plt.ylabel('PM2.5 Value')\n",
        "#         plt.savefig(f'{feature}_vs_PM2.5_scatter.png')\n",
        "#         plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "019GwPMrbmrB"
      },
      "source": [
        "# **Testing:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FjQNzOb6BeQ"
      },
      "outputs": [],
      "source": [
        "def parse2test(data, feats):\n",
        "  x = []\n",
        "  for i in range(90):\n",
        "    x_tmp = data[feats,8*i: 8*i+8]\n",
        "    x.append(x_tmp.reshape(-1,))\n",
        "\n",
        "  # x.shape: (n, 15, 8)\n",
        "  x = np.array(x)\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hs6zgW-_IQFc"
      },
      "outputs": [],
      "source": [
        "def normalize_test_data(df, norm_params):\n",
        "    data_norm = df.copy()\n",
        "\n",
        "    for col, params in norm_params.items():\n",
        "        if col in df.columns:\n",
        "            data_norm[col] = (df[col] - params['mean']) / params['std']\n",
        "\n",
        "    return data_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "m00CNh3QHJP5",
        "outputId": "9455fe69-86b5-48b5-f2fb-445421acf0af"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv('test.csv')\n",
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWff4h-MHQwT"
      },
      "outputs": [],
      "source": [
        "# Testing data preprocessing\n",
        "def test_processing(test_df, norm=False, norm_params=norm_params):\n",
        "    if norm:\n",
        "        if norm_params is None:\n",
        "            raise ValueError(\"norm_params required when norm=True\")\n",
        "\n",
        "        # Apply training normalization parameters to testing data\n",
        "        data_norm = normalize_test_data(test_df, norm_params)\n",
        "        data_values = data_norm.values\n",
        "    else:\n",
        "        # Use raw testing data\n",
        "        data_values = test_df.values\n",
        "\n",
        "    # Common processing steps\n",
        "    test_data = np.transpose(np.array(np.float64(data_values)))\n",
        "    test_x = parse2test(test_data, feats)\n",
        "\n",
        "    return test_x\n",
        "\n",
        "test_x = test_processing(test_df, norm=use_norm, norm_params=norm_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWrfEwaEdO6J"
      },
      "source": [
        "# **Write result as .csv**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqEQ1fZ9-WMO",
        "outputId": "05376f79-917f-4b0e-8b0a-f1cf1a8180d8"
      },
      "outputs": [],
      "source": [
        "with open('my_sol.csv', 'w', newline='') as csvf:\n",
        "  writer = csv.writer(csvf)\n",
        "  writer.writerow(['Id','Predicted'])\n",
        "\n",
        "  print(test_x.shape)\n",
        "  for i in range(int(test_x.shape[0])):\n",
        "    # Prediction of linear regression\n",
        "    prediction = (np.dot(np.reshape(w,-1),test_x[i]) + bias)[0]\n",
        "    writer.writerow([i, prediction])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
