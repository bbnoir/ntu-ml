{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opTIkC9Ct_YS"
   },
   "source": [
    "# ML HW4 Sample Code\n",
    "TODO:\n",
    " - Design your LSTM model\n",
    " - Use unlabelled data (train_nolabel.csv) for Word2Vec training\n",
    "    - Combine labeled + unlabeled data to train better embeddings\n",
    " - Train with labelled data (train_label.csv)\n",
    "    - Optional: Data augmentation\n",
    "    - Optional: Custom loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iun98ffpt_YW"
   },
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OA-FU0ZkuOov",
    "outputId": "74ff2aab-9bc8-4ff5-909f-02ac77bd735f"
   },
   "outputs": [],
   "source": [
    "# !pip install -U gdown -q\n",
    "# !gdown --folder https://drive.google.com/drive/folders/1786AXJRAtqFvWMBeh-bLm4MtU21IQpBg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLz0hckrt_YX"
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8nGkJNjLt_YX",
    "outputId": "f79c749a-8e40-44b9-de8d-bb4be5796823"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# !pip install gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xu_xFrj6kWl"
   },
   "source": [
    "## Set the Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNozLWMB6ocC"
   },
   "outputs": [],
   "source": [
    "# Training Config\n",
    "DEVICE_NUM = 2\n",
    "BATCH_SIZE = 128\n",
    "EPOCH_NUM = 20\n",
    "MAX_POSITIONS_LEN = 500\n",
    "SEED = 2025\n",
    "MODEL_DIR = 'model.pth'\n",
    "lr = 0.001\n",
    "\n",
    "# Set Seed\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# torch.cuda.set_device(0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# RNN Config\n",
    "w2v_config = {'path': 'w2v_model', 'dim': 512}\n",
    "lstm_config = {'hidden_dim': 512, 'num_layers': 2, 'bidirectional': True, 'fix_embedding': True}\n",
    "header_config = {'dropout': 0.3, 'hidden_dim': 1024}\n",
    "assert header_config['hidden_dim'] == lstm_config['hidden_dim'] or header_config['hidden_dim'] == lstm_config['hidden_dim'] * 2\n",
    "\n",
    "train_dataset_dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBV8B54v67K6"
   },
   "source": [
    "## Utils for Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6nMN8TwRt_Ya"
   },
   "outputs": [],
   "source": [
    "def parsing_text(text):\n",
    "    # Explicitly handle None values\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    # Handle NaN values (which often appear as floats in pandas)\n",
    "    if pd.isna(text):\n",
    "        return \"\"  # Convert NaN to an empty string\n",
    "    # Optional: Add data preprocessing (e.g., lowercasing, removing special characters)\n",
    "    text = text.lower()\n",
    "    special_chars = ['!', '?', '(', ')', '[', ']', '{', '}', '\"', \"'\", '_', '/', '\\\\', '@', '#', '$', '%', '^', '&', '*', '+', '=', '<', '>', '~', '`']\n",
    "    for char in special_chars:\n",
    "        text = text.replace(char, '')\n",
    "    return str(text)\n",
    "\n",
    "def load_train_label(path='train_label.csv'):\n",
    "    tra_lb_pd = pd.read_csv(path)\n",
    "    idx = tra_lb_pd['id'].tolist()\n",
    "    text = [parsing_text(s).split() for s in tra_lb_pd['text'].tolist()]\n",
    "    label = tra_lb_pd['label'].tolist()\n",
    "    return idx, text, label\n",
    "\n",
    "def load_train_nolabel(path='train_nolabel.csv'):\n",
    "    tra_nlb_pd = pd.read_csv(path)\n",
    "    text = [parsing_text(s).split() for s in tra_nlb_pd['text'].tolist()]\n",
    "    return text\n",
    "\n",
    "def load_test(path='test.csv'):\n",
    "    test_pd = pd.read_csv(path)\n",
    "    idx = test_pd['id'].tolist()\n",
    "    text = [parsing_text(s).split() for s in test_pd['text'].tolist()]\n",
    "    return idx, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umttnH407F08"
   },
   "source": [
    "## Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_F-tUO37Fn8"
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, sentences, w2v_config):\n",
    "        self.sentences = sentences\n",
    "        self.idx2word = []\n",
    "        self.word2idx = {}\n",
    "        self.embedding_matrix = []\n",
    "        self.build_word2vec(sentences, **w2v_config)\n",
    "\n",
    "    def build_word2vec(self, x, path, dim):\n",
    "        if os.path.isfile(path):\n",
    "            print(\"loading word2vec model ...\")\n",
    "            w2v_model = Word2Vec.load(path)\n",
    "        else:\n",
    "            print(\"training word2vec model ...\")\n",
    "            w2v_model = Word2Vec(x, vector_size=dim, window=5, min_count=2, workers=12, epochs=10, sg=1)\n",
    "            print(\"saving word2vec model ...\")\n",
    "            w2v_model.save(path)\n",
    "\n",
    "        self.embedding_dim = w2v_model.vector_size\n",
    "        for i, word in enumerate(w2v_model.wv.key_to_index):\n",
    "            #e.g. self.word2index['he'] = 1\n",
    "            #e.g. self.index2word[1] = 'he'\n",
    "            #e.g. self.vectors[1] = 'he' vector\n",
    "\n",
    "            self.word2idx[word] = len(self.word2idx)\n",
    "            self.idx2word.append(word)\n",
    "            self.embedding_matrix.append(w2v_model.wv[word])\n",
    "\n",
    "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
    "        self.add_embedding('<PAD>')\n",
    "        self.add_embedding('<UNK>')\n",
    "        print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
    "\n",
    "    def add_embedding(self, word):\n",
    "        vector = torch.empty(1, self.embedding_dim)\n",
    "        torch.nn.init.uniform_(vector)\n",
    "        self.word2idx[word] = len(self.word2idx)\n",
    "        self.idx2word.append(word)\n",
    "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)\n",
    "\n",
    "    def sentence2idx(self, sentence):\n",
    "        sentence_idx = []\n",
    "        for word in sentence:\n",
    "            if word in self.word2idx.keys():\n",
    "                sentence_idx.append(self.word2idx[word])\n",
    "            else:\n",
    "                sentence_idx.append(self.word2idx[\"<UNK>\"])\n",
    "        return torch.LongTensor(sentence_idx)\n",
    "\n",
    "class TwitterDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, id_list, sentences, labels, preprocessor, dropout=0.0):\n",
    "        self.id_list = id_list\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.preprocessor = preprocessor\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is None: return self.id_list[idx], self.preprocessor.sentence2idx(self.sentences[idx])\n",
    "        if self.dropout > 0.0:\n",
    "            sentence_idx = self.preprocessor.sentence2idx(self.sentences[idx])\n",
    "            keep_mask = (torch.rand(len(sentence_idx)) > self.dropout).long()\n",
    "            sentence_idx = sentence_idx * keep_mask + self.preprocessor.word2idx['<PAD>'] * (1 - keep_mask)\n",
    "            return self.id_list[idx], sentence_idx, self.labels[idx]\n",
    "\n",
    "        return self.id_list[idx], self.preprocessor.sentence2idx(self.sentences[idx]), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "        id_list = torch.LongTensor([d[0] for d in data])\n",
    "        lengths = torch.LongTensor([len(d[1]) for d in data])\n",
    "        texts = pad_sequence(\n",
    "            [d[1] for d in data], batch_first=True).contiguous()\n",
    "\n",
    "        if self.labels is None:\n",
    "            return id_list, lengths, texts\n",
    "\n",
    "        labels = torch.FloatTensor([d[2] for d in data])\n",
    "        return id_list, lengths, texts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbZlvBDm7VbW"
   },
   "source": [
    "## RNN Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XNqapXp7Y5E"
   },
   "outputs": [],
   "source": [
    "class LSTM_Backbone(torch.nn.Module):\n",
    "    def __init__(self, embedding, hidden_dim, num_layers, bidirectional, fix_embedding=True):\n",
    "        super(LSTM_Backbone, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(embedding.size(0),embedding.size(1))\n",
    "        self.embedding.weight = torch.nn.Parameter(embedding)\n",
    "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(embedding.size(1), hidden_dim, num_layers=num_layers, \\\n",
    "                                  bidirectional=bidirectional, batch_first=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.embedding(inputs)\n",
    "        x, _ = self.lstm(inputs)\n",
    "        return x\n",
    "\n",
    "class Header(torch.nn.Module):\n",
    "    def __init__(self, dropout, hidden_dim):\n",
    "        super(Header, self).__init__()\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(hidden_dim, 128, kernel_size=3, padding=1),\n",
    "            torch.nn.BatchNorm1d(128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Conv1d(128, 128, kernel_size=3, padding=1),\n",
    "            torch.nn.BatchNorm1d(128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.AdaptiveMaxPool1d(1),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(128, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_length_masks(self, lengths):\n",
    "        # lengths: (batch_size, ) in cuda\n",
    "        ascending = torch.arange(MAX_POSITIONS_LEN)[:lengths.max().item()].unsqueeze(\n",
    "            0).expand(len(lengths), -1).to(lengths.device)\n",
    "        length_masks = (ascending < lengths.unsqueeze(-1)).unsqueeze(-1)\n",
    "        return length_masks\n",
    "\n",
    "    def forward(self, inputs, lengths):\n",
    "        # the input shape should be (N, L, Dâˆ—H)\n",
    "        pad_mask = self._get_length_masks(lengths)\n",
    "        inputs = inputs * pad_mask\n",
    "        \n",
    "        # Permute to (N, C, L) for Conv1d\n",
    "        inputs = inputs.permute(0, 2, 1)\n",
    "        \n",
    "        out = self.classifier(inputs).squeeze()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45SIJnTp7d1A"
   },
   "source": [
    "## Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IE7FGs-O7g-N"
   },
   "outputs": [],
   "source": [
    "def train(train_loader, backbone, header, optimizer, criterion, device, epoch):\n",
    "\n",
    "    total_loss = []\n",
    "    total_acc = []\n",
    "\n",
    "    for i, (idx_list, lengths, texts, labels) in enumerate(train_loader):\n",
    "        lengths, inputs, labels = lengths.to(device), texts.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if not backbone is None:\n",
    "            inputs = backbone(inputs)\n",
    "        soft_predicted = header(inputs, lengths)\n",
    "        loss = criterion(soft_predicted, labels)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(backbone.parameters(), max_norm=1.0) if not backbone is None else None\n",
    "        # torch.nn.utils.clip_grad_norm_(header.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            hard_predicted = (soft_predicted >= 0.5).int()\n",
    "            correct = sum(hard_predicted == labels).item()\n",
    "            batch_size = len(labels)\n",
    "            total_loss.append(loss.item())\n",
    "            total_acc.append(correct * 100 / batch_size)\n",
    "            print('[ Epoch {}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(epoch+1, i+1, len(train_loader), np.mean(total_loss), np.mean(total_acc)), end='\\r')\n",
    "    \n",
    "    return np.mean(total_loss), np.mean(total_acc)\n",
    "\n",
    "\n",
    "def valid(valid_loader, backbone, header, criterion, device, epoch):\n",
    "    backbone.eval()\n",
    "    header.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = []\n",
    "        total_acc = []\n",
    "\n",
    "        for i, (idx_list, lengths, texts, labels) in enumerate(valid_loader):\n",
    "            lengths, inputs, labels = lengths.to(device), texts.to(device), labels.to(device)\n",
    "\n",
    "            if not backbone is None:\n",
    "                inputs = backbone(inputs)\n",
    "            soft_predicted = header(inputs, lengths)\n",
    "            loss = criterion(soft_predicted, labels)\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "            hard_predicted = (soft_predicted >= 0.5).int()\n",
    "            correct = sum(hard_predicted == labels).item()\n",
    "            acc = correct * 100 / len(labels)\n",
    "            total_acc.append(acc)\n",
    "\n",
    "            print('[Validation in epoch {:}] loss:{:.3f} acc:{:.3f}'.format(epoch+1, np.mean(total_loss), np.mean(total_acc)), end='\\r')\n",
    "    backbone.train()\n",
    "    header.train()\n",
    "    return np.mean(total_loss), np.mean(total_acc)\n",
    "\n",
    "\n",
    "def run_training(train_loader, valid_loader, backbone, header, epoch_num, lr, device, model_dir):\n",
    "    best_acc = 0.0\n",
    "    patience = 5\n",
    "    counter = 0\n",
    "\n",
    "    def check_point(backbone, header, loss, acc, model_dir):\n",
    "        nonlocal best_acc\n",
    "        if acc > best_acc:\n",
    "            # Save state_dict instead of full model objects\n",
    "            torch.save({\n",
    "                'backbone_state_dict': backbone.state_dict(),\n",
    "                'header_state_dict': header.state_dict()\n",
    "            }, model_dir)\n",
    "            print(f'New best model saved with accuracy: {acc:.3f}')\n",
    "\n",
    "    def is_stop(loss, acc):\n",
    "        # TODO: Implement early stopping\n",
    "        nonlocal best_acc, counter, patience\n",
    "        if acc > best_acc:\n",
    "            counter = 0\n",
    "            best_acc = acc\n",
    "            return False\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    if backbone is None:\n",
    "        trainable_paras = header.parameters()\n",
    "    else:\n",
    "        trainable_paras = list(backbone.parameters()) + list(header.parameters())\n",
    "\n",
    "    optimizer = torch.optim.AdamW(trainable_paras, lr=lr, weight_decay=1e-3)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "\n",
    "    backbone.train()\n",
    "    header.train()\n",
    "    backbone = backbone.to(device)\n",
    "    header = header.to(device)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        train_loss, train_acc = train(train_loader, backbone, header, optimizer, criterion, device, epoch)\n",
    "        loss, acc = valid(valid_loader, backbone, header, criterion, device, epoch)\n",
    "        print('[Epoch {:}] Train loss:{:.3f} acc:{:.3f} | Val loss:{:.3f} acc:{:.3f} | LR:{:.6f}'.format(\n",
    "            epoch+1, train_loss, train_acc, loss, acc, optimizer.param_groups[0]['lr']))\n",
    "        \n",
    "        check_point(backbone, header, loss, acc, model_dir)\n",
    "        scheduler.step(acc)\n",
    "        \n",
    "        if is_stop(loss, acc):\n",
    "            print(f'Early stopping triggered after {epoch+1} epochs due to no improvement in accuracy for {patience} epochs.')\n",
    "            break\n",
    "    \n",
    "    # Load best model for testing\n",
    "    checkpoint = torch.load(model_dir, weights_only=True)\n",
    "    backbone.load_state_dict(checkpoint['backbone_state_dict'])\n",
    "    header.load_state_dict(checkpoint['header_state_dict'])\n",
    "    print(f'Loaded best model with accuracy: {best_acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYgUWN2B7kCE"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cy89bOzt7le5"
   },
   "outputs": [],
   "source": [
    "def run_testing(test_loader, backbone, header, device, output_path):\n",
    "    with open(output_path, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['id', 'label'])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (idx_list, lengths, texts) in enumerate(test_loader):\n",
    "                lengths, inputs = lengths.to(device), texts.to(device)\n",
    "                if not backbone is None:\n",
    "                    inputs = backbone(inputs)\n",
    "                soft_predicted = header(inputs, lengths)\n",
    "                hard_predicted = (soft_predicted >= 0.5).int()\n",
    "                for i, p in zip(idx_list, hard_predicted):\n",
    "                    writer.writerow([str(i.item()), str(p.item())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTDsL_y67mbt"
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Dataset\n",
    "train_idx, train_label_text, label = load_train_label('dataset/train_label.csv')\n",
    "train_nolabel_text = load_train_nolabel('dataset/train_nolabel.csv')\n",
    "\n",
    "# For sanity check\n",
    "print(train_label_text[:10])\n",
    "print(train_nolabel_text[:10])\n",
    "\n",
    "w2v_sentences = train_label_text + train_nolabel_text\n",
    "\n",
    "# Use labeled data for Word2Vec embeddings (# TODO: Perform unsupervised Learning for w2v)\n",
    "preprocessor = Preprocessor(w2v_sentences, w2v_config)\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "n_splits = 5\n",
    "all_indices = list(range(len(train_label_text)))\n",
    "random.shuffle(all_indices)\n",
    "\n",
    "fold_size = len(all_indices) // n_splits\n",
    "folds = []\n",
    "for i in range(n_splits):\n",
    "    start = i * fold_size\n",
    "    end = (i + 1) * fold_size if i < n_splits - 1 else len(all_indices)\n",
    "    folds.append(all_indices[start:end])\n",
    "\n",
    "test_idx, test_text = load_test('dataset/test.csv')\n",
    "test_dataset = TwitterDataset(test_idx, test_text, None, preprocessor)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                            batch_size = BATCH_SIZE,\n",
    "                                            shuffle = False,\n",
    "                                            collate_fn = test_dataset.collate_fn,\n",
    "                                            num_workers = 8)\n",
    "\n",
    "test_pred_probs = []\n",
    "\n",
    "for fold in range(n_splits):\n",
    "    print(f'\\nFold {fold+1}/{n_splits}')\n",
    "    \n",
    "    val_indices = folds[fold]\n",
    "    train_indices = []\n",
    "    for f in range(n_splits):\n",
    "        if f != fold:\n",
    "            train_indices.extend(folds[f])\n",
    "    \n",
    "    fold_train_idx = [train_idx[i] for i in train_indices]\n",
    "    fold_train_text = [train_label_text[i] for i in train_indices]\n",
    "    fold_train_label = [label[i] for i in train_indices]\n",
    "    \n",
    "    fold_val_idx = [train_idx[i] for i in val_indices]\n",
    "    fold_val_text = [train_label_text[i] for i in val_indices]\n",
    "    fold_val_label = [label[i] for i in val_indices]\n",
    "    \n",
    "    train_dataset = TwitterDataset(fold_train_idx, fold_train_text, fold_train_label, preprocessor, train_dataset_dropout)\n",
    "    valid_dataset = TwitterDataset(fold_val_idx, fold_val_text, fold_val_label, preprocessor)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                                batch_size = BATCH_SIZE,\n",
    "                                                shuffle = True,\n",
    "                                                collate_fn = train_dataset.collate_fn,\n",
    "                                                num_workers = 8)\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset = valid_dataset,\n",
    "                                                batch_size = BATCH_SIZE,\n",
    "                                                shuffle = False,\n",
    "                                                collate_fn = valid_dataset.collate_fn,\n",
    "                                                num_workers = 8)\n",
    "    \n",
    "    backbone = LSTM_Backbone(preprocessor.embedding_matrix, **lstm_config)\n",
    "    header = Header(**header_config)\n",
    "    \n",
    "    model_path = f'model_fold_{fold+1}.pth'\n",
    "    run_training(train_loader, valid_loader, backbone, header, EPOCH_NUM, lr, device, model_path)\n",
    "    \n",
    "    backbone.eval()\n",
    "    header.eval()\n",
    "    \n",
    "    fold_probs = []\n",
    "    with torch.no_grad():\n",
    "        for i, (idx_list, lengths, texts) in enumerate(test_loader):\n",
    "            lengths, inputs = lengths.to(device), texts.to(device)\n",
    "            if not backbone is None:\n",
    "                inputs = backbone(inputs)\n",
    "            soft_predicted = header(inputs, lengths)\n",
    "            fold_probs.extend(soft_predicted.cpu().tolist())\n",
    "    test_pred_probs.append(fold_probs)\n",
    "\n",
    "avg_probs = np.mean(test_pred_probs, axis=0)\n",
    "hard_preds = (avg_probs >= 0.5).astype(int)\n",
    "\n",
    "with open('pred.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'label'])\n",
    "    for i, p in zip(test_idx, hard_preds):\n",
    "        writer.writerow([str(i), str(p)])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
