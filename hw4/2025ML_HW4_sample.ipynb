{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opTIkC9Ct_YS"
   },
   "source": [
    "# ML HW4 Sample Code\n",
    "TODO:\n",
    " - Design your LSTM model\n",
    " - Use unlabelled data (train_nolabel.csv) for Word2Vec training\n",
    "    - Combine labeled + unlabeled data to train better embeddings\n",
    " - Train with labelled data (train_label.csv)\n",
    "    - Optional: Data augmentation\n",
    "    - Optional: Custom loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iun98ffpt_YW"
   },
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OA-FU0ZkuOov",
    "outputId": "74ff2aab-9bc8-4ff5-909f-02ac77bd735f"
   },
   "outputs": [],
   "source": [
    "# !pip install -U gdown -q\n",
    "# !gdown --folder https://drive.google.com/drive/folders/1786AXJRAtqFvWMBeh-bLm4MtU21IQpBg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLz0hckrt_YX"
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8nGkJNjLt_YX",
    "outputId": "f79c749a-8e40-44b9-de8d-bb4be5796823"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# !pip install gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xu_xFrj6kWl"
   },
   "source": [
    "## Set the Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "cNozLWMB6ocC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Training Config\n",
    "DEVICE_NUM = 2\n",
    "BATCH_SIZE = 128\n",
    "EPOCH_NUM = 20\n",
    "MAX_POSITIONS_LEN = 500\n",
    "SEED = 2025\n",
    "MODEL_DIR = 'model.pth'\n",
    "lr = 0.001\n",
    "\n",
    "# Set Seed\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# torch.cuda.set_device(0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# RNN Config\n",
    "w2v_config = {'path': 'w2v_model', 'dim': 512}\n",
    "lstm_config = {'hidden_dim': 512, 'num_layers': 2, 'bidirectional': True, 'fix_embedding': True}\n",
    "header_config = {'dropout': 0.3, 'hidden_dim': 1024}\n",
    "assert header_config['hidden_dim'] == lstm_config['hidden_dim'] or header_config['hidden_dim'] == lstm_config['hidden_dim'] * 2\n",
    "\n",
    "train_dataset_dropout = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBV8B54v67K6"
   },
   "source": [
    "## Utils for Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "6nMN8TwRt_Ya"
   },
   "outputs": [],
   "source": [
    "def parsing_text(text):\n",
    "    # Explicitly handle None values\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    # Handle NaN values (which often appear as floats in pandas)\n",
    "    if pd.isna(text):\n",
    "        return \"\"  # Convert NaN to an empty string\n",
    "    # Optional: Add data preprocessing (e.g., lowercasing, removing special characters)\n",
    "    text = text.lower()\n",
    "    special_chars = ['!', '?', '(', ')', '[', ']', '{', '}', '\"', \"'\", '_', '/', '\\\\', '@', '#', '$', '%', '^', '&', '*', '+', '=', '<', '>', '~', '`']\n",
    "    for char in special_chars:\n",
    "        text = text.replace(char, '')\n",
    "    return str(text)\n",
    "\n",
    "def load_train_label(path='train_label.csv'):\n",
    "    tra_lb_pd = pd.read_csv(path)\n",
    "    idx = tra_lb_pd['id'].tolist()\n",
    "    text = [parsing_text(s).split() for s in tra_lb_pd['text'].tolist()]\n",
    "    label = tra_lb_pd['label'].tolist()\n",
    "    return idx, text, label\n",
    "\n",
    "def load_train_nolabel(path='train_nolabel.csv'):\n",
    "    tra_nlb_pd = pd.read_csv(path)\n",
    "    text = [parsing_text(s).split() for s in tra_nlb_pd['text'].tolist()]\n",
    "    return text\n",
    "\n",
    "def load_test(path='test.csv'):\n",
    "    test_pd = pd.read_csv(path)\n",
    "    idx = test_pd['id'].tolist()\n",
    "    text = [parsing_text(s).split() for s in test_pd['text'].tolist()]\n",
    "    return idx, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umttnH407F08"
   },
   "source": [
    "## Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "M_F-tUO37Fn8"
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, sentences, w2v_config):\n",
    "        self.sentences = sentences\n",
    "        self.idx2word = []\n",
    "        self.word2idx = {}\n",
    "        self.embedding_matrix = []\n",
    "        self.build_word2vec(sentences, **w2v_config)\n",
    "\n",
    "    def build_word2vec(self, x, path, dim):\n",
    "        if os.path.isfile(path):\n",
    "            print(\"loading word2vec model ...\")\n",
    "            w2v_model = Word2Vec.load(path)\n",
    "        else:\n",
    "            print(\"training word2vec model ...\")\n",
    "            w2v_model = Word2Vec(x, vector_size=dim, window=5, min_count=2, workers=12, epochs=10, sg=1)\n",
    "            print(\"saving word2vec model ...\")\n",
    "            w2v_model.save(path)\n",
    "\n",
    "        self.embedding_dim = w2v_model.vector_size\n",
    "        for i, word in enumerate(w2v_model.wv.key_to_index):\n",
    "            #e.g. self.word2index['he'] = 1\n",
    "            #e.g. self.index2word[1] = 'he'\n",
    "            #e.g. self.vectors[1] = 'he' vector\n",
    "\n",
    "            self.word2idx[word] = len(self.word2idx)\n",
    "            self.idx2word.append(word)\n",
    "            self.embedding_matrix.append(w2v_model.wv[word])\n",
    "\n",
    "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
    "        self.add_embedding('<PAD>')\n",
    "        self.add_embedding('<UNK>')\n",
    "        print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
    "\n",
    "    def add_embedding(self, word):\n",
    "        vector = torch.empty(1, self.embedding_dim)\n",
    "        torch.nn.init.uniform_(vector)\n",
    "        self.word2idx[word] = len(self.word2idx)\n",
    "        self.idx2word.append(word)\n",
    "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)\n",
    "\n",
    "    def sentence2idx(self, sentence):\n",
    "        sentence_idx = []\n",
    "        for word in sentence:\n",
    "            if word in self.word2idx.keys():\n",
    "                sentence_idx.append(self.word2idx[word])\n",
    "            else:\n",
    "                sentence_idx.append(self.word2idx[\"<UNK>\"])\n",
    "        return torch.LongTensor(sentence_idx)\n",
    "\n",
    "class TwitterDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, id_list, sentences, labels, preprocessor, dropout=0.0):\n",
    "        self.id_list = id_list\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.preprocessor = preprocessor\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is None: return self.id_list[idx], self.preprocessor.sentence2idx(self.sentences[idx])\n",
    "        if self.dropout > 0.0:\n",
    "            sentence_idx = self.preprocessor.sentence2idx(self.sentences[idx])\n",
    "            keep_mask = (torch.rand(len(sentence_idx)) > self.dropout).long()\n",
    "            sentence_idx = sentence_idx * keep_mask + self.preprocessor.word2idx['<PAD>'] * (1 - keep_mask)\n",
    "            return self.id_list[idx], sentence_idx, self.labels[idx]\n",
    "\n",
    "        return self.id_list[idx], self.preprocessor.sentence2idx(self.sentences[idx]), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "        id_list = torch.LongTensor([d[0] for d in data])\n",
    "        lengths = torch.LongTensor([len(d[1]) for d in data])\n",
    "        texts = pad_sequence(\n",
    "            [d[1] for d in data], batch_first=True).contiguous()\n",
    "\n",
    "        if self.labels is None:\n",
    "            return id_list, lengths, texts\n",
    "\n",
    "        labels = torch.FloatTensor([d[2] for d in data])\n",
    "        return id_list, lengths, texts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbZlvBDm7VbW"
   },
   "source": [
    "## RNN Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "1XNqapXp7Y5E"
   },
   "outputs": [],
   "source": [
    "class LSTM_Backbone(torch.nn.Module):\n",
    "    def __init__(self, embedding, hidden_dim, num_layers, bidirectional, fix_embedding=True):\n",
    "        super(LSTM_Backbone, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(embedding.size(0),embedding.size(1))\n",
    "        self.embedding.weight = torch.nn.Parameter(embedding)\n",
    "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(embedding.size(1), hidden_dim, num_layers=num_layers, \\\n",
    "                                  bidirectional=bidirectional, batch_first=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.embedding(inputs)\n",
    "        x, _ = self.lstm(inputs)\n",
    "        return x\n",
    "\n",
    "class Header(torch.nn.Module):\n",
    "    def __init__(self, dropout, hidden_dim):\n",
    "        super(Header, self).__init__()\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden_dim, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(128, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(128, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_length_masks(self, lengths):\n",
    "        # lengths: (batch_size, ) in cuda\n",
    "        ascending = torch.arange(MAX_POSITIONS_LEN)[:lengths.max().item()].unsqueeze(\n",
    "            0).expand(len(lengths), -1).to(lengths.device)\n",
    "        length_masks = (ascending < lengths.unsqueeze(-1)).unsqueeze(-1)\n",
    "        return length_masks\n",
    "\n",
    "    def forward(self, inputs, lengths):\n",
    "        # the input shape should be (N, L, Dâˆ—H)\n",
    "        pad_mask = self._get_length_masks(lengths)\n",
    "        inputs = inputs * pad_mask\n",
    "        inputs = inputs.sum(dim=1)\n",
    "        out = self.classifier(inputs).squeeze()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45SIJnTp7d1A"
   },
   "source": [
    "## Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "IE7FGs-O7g-N"
   },
   "outputs": [],
   "source": [
    "def train(train_loader, backbone, header, optimizer, criterion, device, epoch):\n",
    "\n",
    "    total_loss = []\n",
    "    total_acc = []\n",
    "\n",
    "    for i, (idx_list, lengths, texts, labels) in enumerate(train_loader):\n",
    "        lengths, inputs, labels = lengths.to(device), texts.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if not backbone is None:\n",
    "            inputs = backbone(inputs)\n",
    "        soft_predicted = header(inputs, lengths)\n",
    "        loss = criterion(soft_predicted, labels)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(backbone.parameters(), max_norm=1.0) if not backbone is None else None\n",
    "        # torch.nn.utils.clip_grad_norm_(header.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            hard_predicted = (soft_predicted >= 0.5).int()\n",
    "            correct = sum(hard_predicted == labels).item()\n",
    "            batch_size = len(labels)\n",
    "            total_loss.append(loss.item())\n",
    "            total_acc.append(correct * 100 / batch_size)\n",
    "            print('[ Epoch {}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(epoch+1, i+1, len(train_loader), np.mean(total_loss), np.mean(total_acc)), end='\\r')\n",
    "    \n",
    "    return np.mean(total_loss), np.mean(total_acc)\n",
    "\n",
    "\n",
    "def valid(valid_loader, backbone, header, criterion, device, epoch):\n",
    "    backbone.eval()\n",
    "    header.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = []\n",
    "        total_acc = []\n",
    "\n",
    "        for i, (idx_list, lengths, texts, labels) in enumerate(valid_loader):\n",
    "            lengths, inputs, labels = lengths.to(device), texts.to(device), labels.to(device)\n",
    "\n",
    "            if not backbone is None:\n",
    "                inputs = backbone(inputs)\n",
    "            soft_predicted = header(inputs, lengths)\n",
    "            loss = criterion(soft_predicted, labels)\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "            hard_predicted = (soft_predicted >= 0.5).int()\n",
    "            correct = sum(hard_predicted == labels).item()\n",
    "            acc = correct * 100 / len(labels)\n",
    "            total_acc.append(acc)\n",
    "\n",
    "            print('[Validation in epoch {:}] loss:{:.3f} acc:{:.3f}'.format(epoch+1, np.mean(total_loss), np.mean(total_acc)), end='\\r')\n",
    "    backbone.train()\n",
    "    header.train()\n",
    "    return np.mean(total_loss), np.mean(total_acc)\n",
    "\n",
    "\n",
    "def run_training(train_loader, valid_loader, backbone, header, epoch_num, lr, device, model_dir):\n",
    "    best_acc = 0.0\n",
    "    patience = 5\n",
    "    counter = 0\n",
    "\n",
    "    def check_point(backbone, header, loss, acc, model_dir):\n",
    "        nonlocal best_acc\n",
    "        if acc > best_acc:\n",
    "            # Save state_dict instead of full model objects\n",
    "            torch.save({\n",
    "                'backbone_state_dict': backbone.state_dict(),\n",
    "                'header_state_dict': header.state_dict()\n",
    "            }, model_dir)\n",
    "            print(f'New best model saved with accuracy: {acc:.3f}')\n",
    "\n",
    "    def is_stop(loss, acc):\n",
    "        # TODO: Implement early stopping\n",
    "        nonlocal best_acc, counter, patience\n",
    "        if acc > best_acc:\n",
    "            counter = 0\n",
    "            best_acc = acc\n",
    "            return False\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    if backbone is None:\n",
    "        trainable_paras = header.parameters()\n",
    "    else:\n",
    "        trainable_paras = list(backbone.parameters()) + list(header.parameters())\n",
    "\n",
    "    optimizer = torch.optim.AdamW(trainable_paras, lr=lr, weight_decay=1e-3)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "\n",
    "    backbone.train()\n",
    "    header.train()\n",
    "    backbone = backbone.to(device)\n",
    "    header = header.to(device)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        train_loss, train_acc = train(train_loader, backbone, header, optimizer, criterion, device, epoch)\n",
    "        loss, acc = valid(valid_loader, backbone, header, criterion, device, epoch)\n",
    "        print('[Epoch {:}] Train loss:{:.3f} acc:{:.3f} | Val loss:{:.3f} acc:{:.3f} | LR:{:.6f}'.format(\n",
    "            epoch+1, train_loss, train_acc, loss, acc, optimizer.param_groups[0]['lr']))\n",
    "        \n",
    "        check_point(backbone, header, loss, acc, model_dir)\n",
    "        scheduler.step(acc)\n",
    "        \n",
    "        if is_stop(loss, acc):\n",
    "            print(f'Early stopping triggered after {epoch+1} epochs due to no improvement in accuracy for {patience} epochs.')\n",
    "            break\n",
    "    \n",
    "    # Load best model for testing\n",
    "    checkpoint = torch.load(model_dir, weights_only=True)\n",
    "    backbone.load_state_dict(checkpoint['backbone_state_dict'])\n",
    "    header.load_state_dict(checkpoint['header_state_dict'])\n",
    "    print(f'Loaded best model with accuracy: {best_acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYgUWN2B7kCE"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "cy89bOzt7le5"
   },
   "outputs": [],
   "source": [
    "def run_testing(test_loader, backbone, header, device, output_path):\n",
    "    with open(output_path, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['id', 'label'])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (idx_list, lengths, texts) in enumerate(test_loader):\n",
    "                lengths, inputs = lengths.to(device), texts.to(device)\n",
    "                if not backbone is None:\n",
    "                    inputs = backbone(inputs)\n",
    "                soft_predicted = header(inputs, lengths)\n",
    "                hard_predicted = (soft_predicted >= 0.5).int()\n",
    "                for i, p in zip(idx_list, hard_predicted):\n",
    "                    writer.writerow([str(i.item()), str(p.item())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTDsL_y67mbt"
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['it', 'was', 'odd', 'not', 'walking', 'with', 'this', 'morning', '.'], ['doesnt', 'understand', 'how', 'a', 'man', 'can', 'pursue', 'a', 'woman', 'so', 'hard', ',', 'but', 'turns', 'around', 'and', 'cheats', '...', 'sigh'], ['i', 'm', 'tired', 'didn', 't', 'do', 'anything', 'all', 'day', 'except', 'went', 'to', 'the', 'craft', 'store', 'to', 'get', 'some', 'hemp', 'string'], ['going', 'to', 'cheddars'], ['haha', 'traci', 's', 'waaaaaasted', '.', 'i', 'miss', 'college'], ['fuck', 'after', 'yesterday', ',', 'whats', 'the', 'point', 'in', 'chasing', 'after', 'her', 'anymore', '....', 'for', 'once', 'in', 'my', 'life', 'i', 'guess', 'its', 'just', 'not', 'ment', 'to', 'be'], ['hey', 'mama', 'how', 'was', 'the', 'vacay'], ['well', 'donnee', 'woo', 'top', '10'], ['how', 'sweet', '.', 'but', 'you', 'dont', 'owe', 'me', 'anything', '.', 'thats', 'wat', 'friends', 'are', 'for', 'right'], ['you', 'may', 'be', 'right', 'i', 'll', 'just', 'have', 'to', 'have', 'my', 'team', 'look', 'to', 'other', 'way', 'as', 'i', 'nod', 'off', 'during', 'the', 'day', 'plus', 'i', 'have', 'to', 'hit', 'the', 'gym']]\n",
      "[['mkhang', 'mlbo', '.', 'dami', 'niang', 'followers', 'ee', '.', 'di', 'q', 'rin', 'naman', 'sia', 'masisisi', '.', 'desperate', 'n', 'kng', 'desperate', ',', 'pero', 'dpt', 'tlga', 'replyn', 'nia', 'q', 'd'], ['don', 't', 'you', 'hate', 'it', 'when', 'you', 'hang', 'on', 'to', 'a', 'seemingly', 'interesting', 'movie', 'to', 'see', 'the', 'ending', 'only', 'to', 'find', 'out', 'that', 'the', 'ending', 'sucks'], ['ok', 'so', 'never', 'went', 'to', 'the', 'movies', 'because', 'friend', 'wasn', 't', 'feeling', 'well', 'but', 'next', 'weekend', '.', 'back', 'to', 'work', 'today', ',', 'wasn', 't', 'too', 'bad', '.'], ['can', 't', 'wait', 'to', 'see', 'diversity', 's', 'performance'], ['i', 'love', 'britney', 'spears', 'haha', 'joey', 'this', 'is', 'what', 'u', 'do', 'go', 'party', 'with', 'eric', 'or', 'do', 'things', 'haha'], ['wish', 'i', 'could', 'call', 'in', 'but', 'i', 'can', 't', 'do', 'blogtalk', 'from', 'work'], ['1', 'more', 'day'], ['nursing', 'celeste', 'with', 'a', 'tummy', 'ache', '.'], ['hates', 'being', 'this', 'burnt', 'ouch'], ['just', 'couldn', 't', 'sleep', 'last', 'night', '.', 'working', '7a', '3p', ',', 'than', 'dinner', 'with', 'megan', '.', 'happy', 'bday', 'jl']]\n",
      "loading word2vec model ...\n",
      "total words: 112614\n",
      "[Epoch 1] Train loss:0.504 acc:75.540 | Val loss:0.415 acc:80.501 | LR:0.001000\n",
      "New best model saved with accuracy: 80.501\n",
      "[Epoch 2] Train loss:0.462 acc:78.272 | Val loss:0.402 acc:81.338 | LR:0.001000\n",
      "New best model saved with accuracy: 81.338\n",
      "[Epoch 3] Train loss:0.448 acc:79.092 | Val loss:0.404 acc:81.610 | LR:0.001000\n",
      "New best model saved with accuracy: 81.610\n",
      "[Epoch 4] Train loss:0.434 acc:79.902 | Val loss:0.394 acc:81.956 | LR:0.001000\n",
      "New best model saved with accuracy: 81.956\n",
      "[Epoch 5] Train loss:0.422 acc:80.445 | Val loss:0.393 acc:82.189 | LR:0.001000\n",
      "New best model saved with accuracy: 82.189\n",
      "[Epoch 6] Train loss:0.411 acc:81.095 | Val loss:0.393 acc:82.450 | LR:0.001000\n",
      "New best model saved with accuracy: 82.450\n",
      "[Epoch 7] Train loss:0.395 acc:82.064 | Val loss:0.399 acc:82.147 | LR:0.001000\n",
      "[Epoch 8] Train loss:0.379 acc:82.868 | Val loss:0.421 acc:81.942 | LR:0.001000\n",
      "[Epoch 9] Train loss:0.361 acc:83.750 | Val loss:0.429 acc:81.356 | LR:0.001000\n",
      "[Epoch 10] Train loss:0.327 acc:85.574 | Val loss:0.448 acc:81.501 | LR:0.000500\n",
      "[Epoch 11] Train loss:0.308 acc:86.453 | Val loss:0.520 acc:81.258 | LR:0.000500\n",
      "Early stopping triggered after 11 epochs due to no improvement in accuracy for 5 epochs.\n",
      "Loaded best model with accuracy: 82.450\n"
     ]
    }
   ],
   "source": [
    "# Split Dataset\n",
    "train_idx, train_label_text, label = load_train_label('dataset/train_label.csv')\n",
    "train_nolabel_text = load_train_nolabel('dataset/train_nolabel.csv')\n",
    "\n",
    "# For sanity check\n",
    "print(train_label_text[:10])\n",
    "print(train_nolabel_text[:10])\n",
    "\n",
    "w2v_sentences = train_label_text + train_nolabel_text\n",
    "\n",
    "# Use labeled data for Word2Vec embeddings (# TODO: Perform unsupervised Learning for w2v)\n",
    "preprocessor = Preprocessor(w2v_sentences, w2v_config)\n",
    "\n",
    "train_idx, valid_idx, train_label_text, valid_label_text, train_label, valid_label = train_test_split(train_idx, train_label_text, label, test_size=0.12)\n",
    "train_dataset, valid_dataset = TwitterDataset(train_idx, train_label_text, train_label, preprocessor, train_dataset_dropout), TwitterDataset(valid_idx, valid_label_text, valid_label, preprocessor)\n",
    "\n",
    "test_idx, test_text = load_test('dataset/test.csv')\n",
    "test_dataset = TwitterDataset(test_idx, test_text, None, preprocessor)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                            batch_size = BATCH_SIZE,\n",
    "                                            shuffle = True,\n",
    "                                            collate_fn = train_dataset.collate_fn,\n",
    "                                            num_workers = 8)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset = valid_dataset,\n",
    "                                            batch_size = BATCH_SIZE,\n",
    "                                            shuffle = False,\n",
    "                                            collate_fn = valid_dataset.collate_fn,\n",
    "                                            num_workers = 8)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                            batch_size = BATCH_SIZE,\n",
    "                                            shuffle = False,\n",
    "                                            collate_fn = test_dataset.collate_fn,\n",
    "                                            num_workers = 8)\n",
    "\n",
    "# Instantiate The Model\n",
    "backbone = LSTM_Backbone(preprocessor.embedding_matrix, **lstm_config)\n",
    "header = Header(**header_config)\n",
    "\n",
    "# Run Training\n",
    "run_training(train_loader, valid_loader, backbone, header, EPOCH_NUM, lr, device, MODEL_DIR)\n",
    "\n",
    "# Run Testing\n",
    "run_testing(test_loader, backbone, header, device, 'pred.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word2vec model ...\n",
      "total words: 112614\n",
      "Total pseudo labels generated: 108258\n"
     ]
    }
   ],
   "source": [
    "train_idx, train_label_text, label = load_train_label('dataset/train_label.csv')\n",
    "train_nolabel_text = load_train_nolabel('dataset/train_nolabel.csv')\n",
    "\n",
    "w2v_sentences = train_label_text + train_nolabel_text\n",
    "preprocessor = Preprocessor(w2v_sentences, w2v_config)\n",
    "\n",
    "nolabel_dataset = TwitterDataset(list(range(len(train_nolabel_text))), train_nolabel_text, None, preprocessor)\n",
    "nolabel_loader = torch.utils.data.DataLoader(nolabel_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=nolabel_dataset.collate_fn, num_workers=8)\n",
    "\n",
    "backbone = LSTM_Backbone(preprocessor.embedding_matrix, **lstm_config)\n",
    "header = Header(**header_config)\n",
    "\n",
    "checkpoint = torch.load(MODEL_DIR, weights_only=True)\n",
    "backbone.load_state_dict(checkpoint['backbone_state_dict'])\n",
    "header.load_state_dict(checkpoint['header_state_dict'])\n",
    "\n",
    "backbone.eval()\n",
    "header.eval()\n",
    "\n",
    "backbone = backbone.to(device)\n",
    "header = header.to(device)\n",
    "\n",
    "soft_threshold = 0.9\n",
    "count = 0\n",
    "pseudo_idx = 200000\n",
    "pseudo_labels = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (idx_list, lengths, texts) in enumerate(nolabel_loader):\n",
    "        lengths, inputs = lengths.to(device), texts.to(device)\n",
    "        if not backbone is None:\n",
    "            inputs = backbone(inputs)\n",
    "        soft_predicted = header(inputs, lengths)\n",
    "        probs = soft_predicted.cpu()\n",
    "        mask_pos = (probs > 0.9) & (random.random() < 0.2)\n",
    "        mask_neg = (probs < 0.1) & (random.random() < 0.2)\n",
    "        batched_pos_idx = idx_list[mask_pos].tolist()\n",
    "        batched_neg_idx = idx_list[mask_neg].tolist()\n",
    "        for idx in batched_pos_idx:\n",
    "            pseudo_labels.append([pseudo_idx, 1])\n",
    "        for idx in batched_neg_idx:\n",
    "            pseudo_labels.append([pseudo_idx, 0])\n",
    "            \n",
    "with open('pseudo_labels.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'label', 'text'])\n",
    "    rows_to_write = []\n",
    "    for idx, label in pseudo_labels:\n",
    "        original_text = ' '.join(train_nolabel_text[idx])\n",
    "        rows_to_write.append([str(pseudo_idx + count), str(label), original_text])\n",
    "        count += 1\n",
    "\n",
    "        if len(rows_to_write) >= 1000:\n",
    "            writer.writerows(rows_to_write)\n",
    "            rows_to_write = []\n",
    "\n",
    "    if rows_to_write:\n",
    "        writer.writerows(rows_to_write)\n",
    "\n",
    "print(f'Total pseudo labels generated: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test with pseudo labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word2vec model ...\n",
      "total words: 112614\n",
      "[Epoch 1] Train loss:0.584 acc:65.989 | Val loss:0.420 acc:80.687 | LR:0.001000\n",
      "New best model saved with accuracy: 80.687\n",
      "[Epoch 2] Train loss:0.557 acc:67.907 | Val loss:0.416 acc:81.694 | LR:0.001000\n",
      "New best model saved with accuracy: 81.694\n",
      "[Epoch 3] Train loss:0.549 acc:68.370 | Val loss:0.397 acc:82.331 | LR:0.001000\n",
      "New best model saved with accuracy: 82.331\n",
      "[Epoch 4] Train loss:0.541 acc:68.892 | Val loss:0.391 acc:82.332 | LR:0.001000\n",
      "New best model saved with accuracy: 82.332\n",
      "[Epoch 5] Train loss:0.533 acc:69.290 | Val loss:0.387 acc:82.585 | LR:0.001000\n",
      "New best model saved with accuracy: 82.585\n",
      "[Epoch 6] Train loss:0.524 acc:69.781 | Val loss:0.397 acc:82.541 | LR:0.001000\n",
      "[Epoch 7] Train loss:0.515 acc:70.258 | Val loss:0.407 acc:82.109 | LR:0.001000\n",
      "[Epoch 8] Train loss:0.504 acc:70.830 | Val loss:0.426 acc:82.148 | LR:0.001000\n",
      "[Epoch 9] Train loss:0.482 acc:71.928 | Val loss:0.435 acc:81.857 | LR:0.000500\n",
      "[Epoch 10] Train loss:0.472 acc:72.399 | Val loss:0.488 acc:81.587 | LR:0.000500\n",
      "Early stopping triggered after 10 epochs due to no improvement in accuracy for 5 epochs.\n",
      "Loaded best model with accuracy: 82.585\n"
     ]
    }
   ],
   "source": [
    "# Split Dataset\n",
    "train_idx, train_label_text, label = load_train_label('dataset/train_label.csv')\n",
    "pseudo_idx, pseudo_label_text, pseudo_label = load_train_label('pseudo_labels.csv')\n",
    "\n",
    "train_nolabel_text = load_train_nolabel('dataset/train_nolabel.csv')\n",
    "\n",
    "w2v_sentences = train_label_text + train_nolabel_text\n",
    "\n",
    "# Use labeled data for Word2Vec embeddings (# TODO: Perform unsupervised Learning for w2v)\n",
    "preprocessor = Preprocessor(w2v_sentences, w2v_config)\n",
    "\n",
    "train_idx, valid_idx, train_label_text, valid_label_text, train_label, valid_label = train_test_split(train_idx, train_label_text, label, test_size=0.12)\n",
    "train_idx += pseudo_idx\n",
    "train_label_text += pseudo_label_text\n",
    "train_label += pseudo_label\n",
    "train_dataset, valid_dataset = TwitterDataset(train_idx, train_label_text, train_label, preprocessor, train_dataset_dropout), TwitterDataset(valid_idx, valid_label_text, valid_label, preprocessor)\n",
    "\n",
    "test_idx, test_text = load_test('dataset/test.csv')\n",
    "test_dataset = TwitterDataset(test_idx, test_text, None, preprocessor)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                            batch_size = BATCH_SIZE,\n",
    "                                            shuffle = True,\n",
    "                                            collate_fn = train_dataset.collate_fn,\n",
    "                                            num_workers = 8)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset = valid_dataset,\n",
    "                                            batch_size = BATCH_SIZE,\n",
    "                                            shuffle = False,\n",
    "                                            collate_fn = valid_dataset.collate_fn,\n",
    "                                            num_workers = 8)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                            batch_size = BATCH_SIZE,\n",
    "                                            shuffle = False,\n",
    "                                            collate_fn = test_dataset.collate_fn,\n",
    "                                            num_workers = 8)\n",
    "\n",
    "# Instantiate The Model\n",
    "backbone = LSTM_Backbone(preprocessor.embedding_matrix, **lstm_config)\n",
    "header = Header(**header_config)\n",
    "\n",
    "# Run Training\n",
    "MODEL_DIR2 = 'model_with_pseudo.pth'\n",
    "run_training(train_loader, valid_loader, backbone, header, EPOCH_NUM, lr, device, MODEL_DIR2)\n",
    "\n",
    "# Run Testing\n",
    "run_testing(test_loader, backbone, header, device, 'pred.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
