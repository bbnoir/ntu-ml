{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opTIkC9Ct_YS"
      },
      "source": [
        "# ML HW4 Sample Code\n",
        "TODO:\n",
        " - Design your LSTM model\n",
        " - Use unlabelled data (train_nolabel.csv) for Word2Vec training\n",
        "    - Combine labeled + unlabeled data to train better embeddings\n",
        " - Train with labelled data (train_label.csv)\n",
        "    - Optional: Data augmentation\n",
        "    - Optional: Custom loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iun98ffpt_YW"
      },
      "source": [
        "## Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OA-FU0ZkuOov",
        "outputId": "74ff2aab-9bc8-4ff5-909f-02ac77bd735f"
      },
      "outputs": [],
      "source": [
        "# !pip install -U gdown -q\n",
        "# !gdown --folder https://drive.google.com/drive/folders/1786AXJRAtqFvWMBeh-bLm4MtU21IQpBg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLz0hckrt_YX"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nGkJNjLt_YX",
        "outputId": "f79c749a-8e40-44b9-de8d-bb4be5796823"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# !pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xu_xFrj6kWl"
      },
      "source": [
        "## Set the Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNozLWMB6ocC"
      },
      "outputs": [],
      "source": [
        "# Training Config\n",
        "DEVICE_NUM = 2\n",
        "BATCH_SIZE = 128\n",
        "EPOCH_NUM = 20\n",
        "MAX_POSITIONS_LEN = 500\n",
        "SEED = 2025\n",
        "MODEL_DIR = 'model.pth'\n",
        "lr = 0.001\n",
        "\n",
        "# Set Seed\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# torch.cuda.set_device(0)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# RNN Config\n",
        "w2v_config = {'path': 'w2v_model', 'dim': 256}\n",
        "lstm_config = {'hidden_dim': 256, 'num_layers': 2, 'bidirectional': True, 'fix_embedding': True}\n",
        "header_config = {'dropout': 0.5, 'hidden_dim': 512}\n",
        "assert header_config['hidden_dim'] == lstm_config['hidden_dim'] or header_config['hidden_dim'] == lstm_config['hidden_dim'] * 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBV8B54v67K6"
      },
      "source": [
        "## Utils for Datasets and Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nMN8TwRt_Ya"
      },
      "outputs": [],
      "source": [
        "def parsing_text(text):\n",
        "    # Explicitly handle None values\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "    # Handle NaN values (which often appear as floats in pandas)\n",
        "    if pd.isna(text):\n",
        "        return \"\"  # Convert NaN to an empty string\n",
        "    # Optional: Add data preprocessing (e.g., lowercasing, removing special characters)\n",
        "    return str(text)\n",
        "\n",
        "def load_train_label(path='train_label.csv'):\n",
        "    tra_lb_pd = pd.read_csv(path)\n",
        "    idx = tra_lb_pd['id'].tolist()\n",
        "    text = [parsing_text(s).split(' ') for s in tra_lb_pd['text'].tolist()]\n",
        "    label = tra_lb_pd['label'].tolist()\n",
        "    return idx, text, label\n",
        "\n",
        "def load_train_nolabel(path='train_nolabel.csv'):\n",
        "    tra_nlb_pd = pd.read_csv(path)\n",
        "    text = [parsing_text(s).split(' ') for s in tra_nlb_pd['text'].tolist()]\n",
        "    return text\n",
        "\n",
        "def load_test(path='test.csv'):\n",
        "    test_pd = pd.read_csv(path)\n",
        "    idx = test_pd['id'].tolist()\n",
        "    text = [parsing_text(s).split(' ') for s in test_pd['text'].tolist()]\n",
        "    return idx, text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umttnH407F08"
      },
      "source": [
        "## Datasets and Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_F-tUO37Fn8"
      },
      "outputs": [],
      "source": [
        "class Preprocessor:\n",
        "    def __init__(self, sentences, w2v_config):\n",
        "        self.sentences = sentences\n",
        "        self.idx2word = []\n",
        "        self.word2idx = {}\n",
        "        self.embedding_matrix = []\n",
        "        self.build_word2vec(sentences, **w2v_config)\n",
        "\n",
        "    def build_word2vec(self, x, path, dim):\n",
        "        if os.path.isfile(path):\n",
        "            print(\"loading word2vec model ...\")\n",
        "            w2v_model = Word2Vec.load(path)\n",
        "        else:\n",
        "            print(\"training word2vec model ...\")\n",
        "            w2v_model = Word2Vec(x, vector_size=dim, window=5, min_count=2, workers=12, epochs=2, sg=1)\n",
        "            print(\"saving word2vec model ...\")\n",
        "            w2v_model.save(path)\n",
        "\n",
        "        self.embedding_dim = w2v_model.vector_size\n",
        "        for i, word in enumerate(w2v_model.wv.key_to_index):\n",
        "            #e.g. self.word2index['he'] = 1\n",
        "            #e.g. self.index2word[1] = 'he'\n",
        "            #e.g. self.vectors[1] = 'he' vector\n",
        "\n",
        "            self.word2idx[word] = len(self.word2idx)\n",
        "            self.idx2word.append(word)\n",
        "            self.embedding_matrix.append(w2v_model.wv[word])\n",
        "\n",
        "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
        "        self.add_embedding('<PAD>')\n",
        "        self.add_embedding('<UNK>')\n",
        "        print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
        "\n",
        "    def add_embedding(self, word):\n",
        "        vector = torch.empty(1, self.embedding_dim)\n",
        "        torch.nn.init.uniform_(vector)\n",
        "        self.word2idx[word] = len(self.word2idx)\n",
        "        self.idx2word.append(word)\n",
        "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)\n",
        "\n",
        "    def sentence2idx(self, sentence):\n",
        "        sentence_idx = []\n",
        "        for word in sentence:\n",
        "            if word in self.word2idx.keys():\n",
        "                sentence_idx.append(self.word2idx[word])\n",
        "            else:\n",
        "                sentence_idx.append(self.word2idx[\"<UNK>\"])\n",
        "        return torch.LongTensor(sentence_idx)\n",
        "\n",
        "class TwitterDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, id_list, sentences, labels, preprocessor):\n",
        "        self.id_list = id_list\n",
        "        self.sentences = sentences\n",
        "        self.labels = labels\n",
        "        self.preprocessor = preprocessor\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.labels is None: return self.id_list[idx], self.preprocessor.sentence2idx(self.sentences[idx])\n",
        "        return self.id_list[idx], self.preprocessor.sentence2idx(self.sentences[idx]), self.labels[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def collate_fn(self, data):\n",
        "        id_list = torch.LongTensor([d[0] for d in data])\n",
        "        lengths = torch.LongTensor([len(d[1]) for d in data])\n",
        "        texts = pad_sequence(\n",
        "            [d[1] for d in data], batch_first=True).contiguous()\n",
        "\n",
        "        if self.labels is None:\n",
        "            return id_list, lengths, texts\n",
        "\n",
        "        labels = torch.FloatTensor([d[2] for d in data])\n",
        "        return id_list, lengths, texts, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbZlvBDm7VbW"
      },
      "source": [
        "## RNN Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XNqapXp7Y5E"
      },
      "outputs": [],
      "source": [
        "class LSTM_Backbone(torch.nn.Module):\n",
        "    def __init__(self, embedding, hidden_dim, num_layers, bidirectional, fix_embedding=True):\n",
        "        super(LSTM_Backbone, self).__init__()\n",
        "        self.embedding = torch.nn.Embedding(embedding.size(0),embedding.size(1))\n",
        "        self.embedding.weight = torch.nn.Parameter(embedding)\n",
        "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
        "\n",
        "        self.lstm = torch.nn.LSTM(embedding.size(1), hidden_dim, num_layers=num_layers, \\\n",
        "                                  bidirectional=bidirectional, batch_first=True)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        inputs = self.embedding(inputs)\n",
        "        x, _ = self.lstm(inputs)\n",
        "        return x\n",
        "\n",
        "class Header(torch.nn.Module):\n",
        "    def __init__(self, dropout, hidden_dim):\n",
        "        super(Header, self).__init__()\n",
        "        # TODO: You can design your classifier module\n",
        "        self.classifier = torch.nn.Sequential(torch.nn.Dropout(dropout),\n",
        "                                         torch.nn.Linear(hidden_dim, 1),\n",
        "                                         torch.nn.Sigmoid())\n",
        "\n",
        "    @ torch.no_grad()\n",
        "    def _get_length_masks(self, lengths):\n",
        "        # lengths: (batch_size, ) in cuda\n",
        "        ascending = torch.arange(MAX_POSITIONS_LEN)[:lengths.max().item()].unsqueeze(\n",
        "            0).expand(len(lengths), -1).to(lengths.device)\n",
        "        length_masks = (ascending < lengths.unsqueeze(-1)).unsqueeze(-1)\n",
        "        return length_masks\n",
        "\n",
        "    def forward(self, inputs, lengths):\n",
        "        # the input shape should be (N, L, Dâˆ—H)\n",
        "        pad_mask = self._get_length_masks(lengths)\n",
        "        inputs = inputs * pad_mask\n",
        "        inputs = inputs.sum(dim=1)\n",
        "        out = self.classifier(inputs).squeeze()\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45SIJnTp7d1A"
      },
      "source": [
        "## Training & Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "IE7FGs-O7g-N"
      },
      "outputs": [],
      "source": [
        "def train(train_loader, backbone, header, optimizer, criterion, device, epoch):\n",
        "\n",
        "    total_loss = []\n",
        "    total_acc = []\n",
        "\n",
        "    for i, (idx_list, lengths, texts, labels) in enumerate(train_loader):\n",
        "        lengths, inputs, labels = lengths.to(device), texts.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        if not backbone is None:\n",
        "            inputs = backbone(inputs)\n",
        "        soft_predicted = header(inputs, lengths)\n",
        "        loss = criterion(soft_predicted, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            hard_predicted = (soft_predicted >= 0.5).int()\n",
        "            correct = sum(hard_predicted == labels).item()\n",
        "            batch_size = len(labels)\n",
        "            print('[ Epoch {}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(epoch+1,i+1, len(train_loader), loss.item(), correct * 100 / batch_size), end='\\r')\n",
        "\n",
        "\n",
        "def valid(valid_loader, backbone, header, criterion, device, epoch):\n",
        "    backbone.eval()\n",
        "    header.eval()\n",
        "    with torch.no_grad():\n",
        "        total_loss = []\n",
        "        total_acc = []\n",
        "\n",
        "        for i, (idx_list, lengths, texts, labels) in enumerate(valid_loader):\n",
        "            lengths, inputs, labels = lengths.to(device), texts.to(device), labels.to(device)\n",
        "\n",
        "            if not backbone is None:\n",
        "                inputs = backbone(inputs)\n",
        "            soft_predicted = header(inputs, lengths)\n",
        "            loss = criterion(soft_predicted, labels)\n",
        "            total_loss.append(loss.item())\n",
        "\n",
        "            hard_predicted = (soft_predicted >= 0.5).int()\n",
        "            correct = sum(hard_predicted == labels).item()\n",
        "            acc = correct * 100 / len(labels)\n",
        "            total_acc.append(acc)\n",
        "\n",
        "            print('[Validation in epoch {:}] loss:{:.3f} acc:{:.3f}'.format(epoch+1, np.mean(total_loss), np.mean(total_acc)), end='\\r')\n",
        "    backbone.train()\n",
        "    header.train()\n",
        "    return np.mean(total_loss), np.mean(total_acc)\n",
        "\n",
        "\n",
        "def run_training(train_loader, valid_loader, backbone, header, epoch_num, lr, device, model_dir):\n",
        "    best_acc = 0.0\n",
        "    patience = 5\n",
        "    counter = 0\n",
        "\n",
        "    def check_point(backbone, header, loss, acc, model_dir):\n",
        "        # Checkpoint saving only when accuracy improves\n",
        "        nonlocal best_acc\n",
        "        if acc > best_acc:\n",
        "            torch.save({'backbone': backbone, 'header': header}, model_dir)\n",
        "            print(f'New best model saved with accuracy: {best_acc:.3f}')\n",
        "\n",
        "    def is_stop(loss, acc):\n",
        "        # TODO: Implement early stopping\n",
        "        nonlocal best_acc, counter, patience\n",
        "        if acc > best_acc:\n",
        "            counter = 0\n",
        "            best_acc = acc\n",
        "            return False\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    if backbone is None:\n",
        "        trainable_paras = header.parameters()\n",
        "    else:\n",
        "        trainable_paras = list(backbone.parameters()) + list(header.parameters())\n",
        "\n",
        "    optimizer = torch.optim.Adam(trainable_paras, lr=lr)\n",
        "\n",
        "    backbone.train()\n",
        "    header.train()\n",
        "    backbone = backbone.to(device)\n",
        "    header = header.to(device)\n",
        "    criterion = torch.nn.BCELoss()\n",
        "    for epoch in range(epoch_num):\n",
        "        train(train_loader, backbone, header, optimizer, criterion, device, epoch)\n",
        "        loss, acc = valid(valid_loader, backbone, header, criterion, device, epoch)\n",
        "        print('[Validation in epoch {:}] loss:{:.3f} acc:{:.3f} '.format(epoch+1, loss, acc))\n",
        "        check_point(backbone, header, loss, acc, model_dir) # Call checkpoint after validation\n",
        "        if is_stop(loss, acc):\n",
        "            print(f'Early stopping triggered after {epoch+1} epochs due to no improvement in accuracy for {patience} epochs.')\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYgUWN2B7kCE"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "cy89bOzt7le5"
      },
      "outputs": [],
      "source": [
        "def run_testing(test_loader, backbone, header, device, output_path):\n",
        "    with open(output_path, 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['id', 'label'])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, (idx_list, lengths, texts) in enumerate(test_loader):\n",
        "                lengths, inputs = lengths.to(device), texts.to(device)\n",
        "                if not backbone is None:\n",
        "                    inputs = backbone(inputs)\n",
        "                soft_predicted = header(inputs, lengths)\n",
        "                hard_predicted = (soft_predicted >= 0.5).int()\n",
        "                for i, p in zip(idx_list, hard_predicted):\n",
        "                    writer.writerow([str(i.item()), str(p.item())])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTDsL_y67mbt"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['it', 'was', 'odd', 'not', 'walking', 'with', 'this', 'morning', '.'], ['doesnt', 'understand', 'how', 'a', 'man', 'can', 'pursue', 'a', 'woman', 'so', 'hard', ',', 'but', 'turns', 'around', 'and', 'cheats', '...', 'sigh'], ['i', \"'\", 'm', 'tired', 'didn', \"'\", 't', 'do', 'anything', 'all', 'day', '!', 'except', 'went', 'to', 'the', 'craft', 'store', 'to', 'get', 'some', 'hemp', 'string'], ['going', 'to', 'cheddars'], ['haha', 'traci', \"'\", 's', 'waaaaaasted', '.', 'i', 'miss', 'college'], ['fuck', 'after', 'yesterday', ',', 'whats', 'the', 'point', 'in', 'chasing', 'after', 'her', 'anymore', '....', 'for', 'once', 'in', 'my', 'life', 'i', 'guess', 'its', 'just', 'not', 'ment', 'to', 'be'], ['hey', 'mama', 'how', 'was', 'the', 'vacay', '?'], ['well', 'donnee', '!!', 'woo', 'top', '10'], ['how', 'sweet', '.', 'but', 'you', 'dont', 'owe', 'me', 'anything', '.', 'thats', 'wat', 'friends', 'are', 'for', 'right', '?'], ['you', 'may', 'be', 'right', '!', 'i', \"'\", 'll', 'just', 'have', 'to', 'have', 'my', 'team', 'look', 'to', 'other', 'way', 'as', 'i', 'nod', 'off', 'during', 'the', 'day', '!', 'plus', 'i', 'have', 'to', 'hit', 'the', 'gym']]\n",
            "[['mkhang', 'mlbo', '.', 'dami', 'niang', 'followers', 'ee', '.', 'di', 'q', 'rin', 'naman', 'sia', 'masisisi', '.', 'desperate', 'n', 'kng', 'desperate', ',', 'pero', 'dpt', 'tlga', 'replyn', 'nia', 'q', '=', 'd'], ['don', \"'\", 't', 'you', 'hate', 'it', 'when', 'you', 'hang', 'on', 'to', 'a', 'seemingly', 'interesting', 'movie', 'to', 'see', 'the', 'ending', 'only', 'to', 'find', 'out', 'that', 'the', 'ending', 'sucks', '?'], ['ok', 'so', 'never', 'went', 'to', 'the', 'movies', 'because', 'friend', 'wasn', \"'\", 't', 'feeling', 'well', 'but', 'next', 'weekend', '.', 'back', 'to', 'work', 'today', ',', 'wasn', \"'\", 't', 'too', 'bad', '.'], ['can', \"'\", 't', 'wait', 'to', 'see', 'diversity', \"'\", 's', 'performance', '!'], ['i', 'love', 'britney', 'spears', 'haha', 'joey', 'this', 'is', 'what', 'u', 'do', 'go', 'party', 'with', 'eric', 'or', 'do', 'things', 'haha'], ['wish', 'i', 'could', 'call', 'in', 'but', 'i', 'can', \"'\", 't', 'do', 'blogtalk', 'from', 'work'], ['1', 'more', 'day', '!'], ['nursing', 'celeste', 'with', 'a', 'tummy', 'ache', '.'], ['hates', 'being', 'this', 'burnt', '!!', 'ouch'], ['just', 'couldn', \"'\", 't', 'sleep', 'last', 'night', '.', 'working', '7a', '3p', ',', 'than', 'dinner', 'with', 'megan', '.', 'happy', 'bday', 'jl', '!']]\n",
            "loading word2vec model ...\n",
            "total words: 113351\n",
            "[Validation in epoch 1] loss:0.431 acc:80.026 \n",
            "New best model saved with accuracy: 0.000\n",
            "[Validation in epoch 2] loss:0.428 acc:80.081 \n",
            "New best model saved with accuracy: 80.026\n",
            "[Validation in epoch 3] loss:0.402 acc:81.608 \n",
            "New best model saved with accuracy: 80.081\n",
            "[Validation in epoch 4] loss:0.410 acc:81.442 \n",
            "[Validation in epoch 5] loss:0.389 acc:82.310 \n",
            "New best model saved with accuracy: 81.608\n",
            "[Validation in epoch 6] loss:0.402 acc:82.393 \n",
            "New best model saved with accuracy: 82.310\n",
            "[Validation in epoch 7] loss:0.391 acc:82.261 \n",
            "[Validation in epoch 8] loss:0.402 acc:82.158 \n",
            "[Validation in epoch 9] loss:0.429 acc:81.940 \n",
            "[Validation in epoch 10] loss:0.470 acc:81.388 \n",
            "[Validation in epoch 11] loss:0.516 acc:80.895 \n",
            "Early stopping triggered after 11 epochs due to no improvement in accuracy for 5 epochs.\n"
          ]
        }
      ],
      "source": [
        "# Split Dataset\n",
        "train_idx, train_label_text, label = load_train_label('dataset/train_label.csv')\n",
        "train_nolabel_text = load_train_nolabel('dataset/train_nolabel.csv')\n",
        "\n",
        "# For sanity check\n",
        "print(train_label_text[:10])\n",
        "print(train_nolabel_text[:10])\n",
        "\n",
        "w2v_sentences = train_label_text + train_nolabel_text\n",
        "\n",
        "# Use labeled data for Word2Vec embeddings (# TODO: Perform unsupervised Learning for w2v)\n",
        "preprocessor = Preprocessor(w2v_sentences, w2v_config)\n",
        "\n",
        "train_idx, valid_idx, train_label_text, valid_label_text, train_label, valid_label = train_test_split(train_idx, train_label_text, label, test_size=0.12)\n",
        "train_dataset, valid_dataset = TwitterDataset(train_idx, train_label_text, train_label, preprocessor), TwitterDataset(valid_idx, valid_label_text, valid_label, preprocessor)\n",
        "\n",
        "test_idx, test_text = load_test('dataset/test.csv')\n",
        "test_dataset = TwitterDataset(test_idx, test_text, None, preprocessor)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                            batch_size = BATCH_SIZE,\n",
        "                                            shuffle = True,\n",
        "                                            collate_fn = train_dataset.collate_fn,\n",
        "                                            num_workers = 8)\n",
        "valid_loader = torch.utils.data.DataLoader(dataset = valid_dataset,\n",
        "                                            batch_size = BATCH_SIZE,\n",
        "                                            shuffle = False,\n",
        "                                            collate_fn = valid_dataset.collate_fn,\n",
        "                                            num_workers = 8)\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                            batch_size = BATCH_SIZE,\n",
        "                                            shuffle = False,\n",
        "                                            collate_fn = test_dataset.collate_fn,\n",
        "                                            num_workers = 8)\n",
        "\n",
        "# Instantiate The Model\n",
        "backbone = LSTM_Backbone(preprocessor.embedding_matrix, **lstm_config)\n",
        "header = Header(**header_config)\n",
        "\n",
        "# Run Training\n",
        "run_training(train_loader, valid_loader, backbone, header, EPOCH_NUM, lr, device, MODEL_DIR)\n",
        "\n",
        "# Run Testing\n",
        "run_testing(test_loader, backbone, header, device, 'pred.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
